"""
This dataset was build for development and evaluation of automatic speech recognizer techniques for English ATC data. Note: The dataset is considered as beta version and will be updated in the near future (some transcript fine tuning may happen). The dataset consists of English coming from LKTB, LKPR, LZIB, LSGS, LSZH, LSZB and YSSY airports The length of audio is 1.10 hours in total. We provided audio (wav format), English automatic transcript generated by an ASR and info file with meta information and nearby callsigns.

See https://www.atco2.org/data for more details.
"""

import hashlib
import logging
import os
import re
import shutil
import tarfile
import unicodedata
import xml.etree.ElementTree as ET
from pathlib import Path
from typing import Dict, Optional, Union

from tqdm.auto import tqdm

from lhotse import validate_recordings_and_supervisions
from lhotse.audio import Recording, RecordingSet
from lhotse.supervision import SupervisionSegment, SupervisionSet
from lhotse.utils import Pathlike, resumable_download, safe_extract


def download_atco2(
    target_dir: Pathlike = ".", force_download: Optional[bool] = False
) -> Path:
    target_dir = Path(target_dir)
    target_dir.mkdir(parents=True, exist_ok=True)
    dataset_name = "ATCO2-ASRdataset-v1_beta"
    tar_path = target_dir / f"{dataset_name}.tgz"
    corpus_dir = target_dir / dataset_name
    completed_detector = corpus_dir / ".completed"
    if completed_detector.is_file():
        logging.info(f"Skipping {dataset_name} because {completed_detector} exists.")
        return corpus_dir
    resumable_download(
        f"https://www.replaywell.com/atco2/download/{dataset_name}.tgz",
        filename=tar_path,
        completed_file_size=132304362,
        force_download=force_download,
    )
    if (
        hashlib.md5(open(tar_path, "rb").read()).hexdigest()
        != "8d604c1a070e2a70a22ef6106aced0cd"
    ):
        raise RuntimeError("MD5 checksum does not match")
    shutil.rmtree(corpus_dir, ignore_errors=True)
    with tarfile.open(tar_path) as tar:
        safe_extract(tar, path=target_dir)
    completed_detector.touch()
    return corpus_dir


def strip_accents(s):
    return "".join(
        c for c in unicodedata.normalize("NFD", s) if unicodedata.category(c) != "Mn"
    )


FIX_TYPOS = {
    "clearence": "CLEARANCE",
    "exprite": "EXPEDITE",
    "niner": "NINE",  # consistency with ATCOSIM & UWB-ATCC
    "Niner": "NINE",  # consistency with ATCOSIM & UWB-ATCC
    "ninty": "NINETY",
    "orbid": "ORBIT",
    "QHN": "Q N H",
    "rodger": "ROGER",
    "startup": "START UP",
    "takeoff": "TAKE OFF",
    "tourning": "TURNING",
    "vilco": "WILCO",
    "X-ray": "XRAY",
}

DASH_TO_SPACE = (
    "bye-bye",
    "left-hand",
    "Saint-Martin",
    "Val-d'Annivier",
)

INDIVIDUALLY_PRONOUNCED = (
    "ARP",
    "CSA",
    "CTR",
    "DME",
    "IFR",
    "ILS",
    "KLM",
    "PG",
    "QNH",
    "RNP",
    "TMA",
    "UPS",
    "VFR",
)

COLLAPSE_WORDS = (("VIET NAM", "VIETNAM"),)

CALLSIGN_PATTERN = re.compile(r"\[#callsign](.*?)\[\/#callsign]")
COMMAND_PATTERN = re.compile(r"\[#command](.*?)\[\/#command]")
VALUE_PATTERN = re.compile(r"\[#value](.*?)\[\/#value]")
UNNAMED_PATTERN = re.compile(r"\[#unnamed](.*?)\[\/#unnamed]")
PARENTHESIS1 = re.compile(r"\(-[a-z]+\)")
PARENTHESIS2 = re.compile(r"\([A-Za-z]+-\)")
NE_PATTERN = re.compile(r"\[NE](.*?)\[\/NE]")
NON_ENGLISH_PATTERN = re.compile(r"\[#NONENGLISH](.*?)\[\/#NONENGLISH]")
NE_CZECH_PATTERN = re.compile(r"\[NE CZECH](.*?)\[[\/\\]NE]")
NE_SLOVAK_PATTERN = re.compile(r"\[NE SLOVAK](.*?)\[\/NE]")
NE_FRENCH_PATTERN = re.compile(r"\[NE-FRENCH](.*?)\[\/NE]")
NE_GERMAN_PATTERN = re.compile(r"\[NE[- ]GERMAN](.*?)\[\/NE]")
INTERRUPTED_PATTERN1 = re.compile(r"(\w+-)[ $]")
WHITESPACE_PATTERN = re.compile(r"  +")


def text_normalize(
    text: str,
    foreign_sym: str,  # When None, will output foreign words
    partial_sym: str,  # When None, will output partial words
    hesitation_sym: str,
    unknown_sym: str,
):

    text = CALLSIGN_PATTERN.sub(r"\1", text)
    text = COMMAND_PATTERN.sub(r"\1", text)
    text = VALUE_PATTERN.sub(r"\1", text)
    text = UNNAMED_PATTERN.sub(r"\1", text)
    text = PARENTHESIS1.sub("", text)
    text = PARENTHESIS2.sub("", text)

    text = strip_accents(text)

    # perform this fix before split()
    text = text.replace("[hes ]", "[hes]")
    text = text.replace("pre- approved", "preapproved")
    text = text.replace("schone_tag", "schone tag")

    result = []
    for w in text.split():
        if w in FIX_TYPOS:
            result.append(FIX_TYPOS[w])
        elif w in DASH_TO_SPACE:
            result.append(w.replace("-", " ").upper())
        elif w in ("[hes]", "[HES]"):
            result.append(hesitation_sym)
        elif w in ("[ukn]", "[XT]"):
            result.append(unknown_sym)
        elif w in ("[noise]", "[spk]"):
            True
        elif w in INDIVIDUALLY_PRONOUNCED:
            result.append(" ".join([*w]))
        else:
            result.append(w.upper())
    text = " ".join(result)

    if foreign_sym is None:
        text = NE_PATTERN.sub(r"\1", text)
        text = NON_ENGLISH_PATTERN.sub(r"\1", text)
        text = NE_CZECH_PATTERN.sub(r"\1", text)
        text = NE_SLOVAK_PATTERN.sub(r"\1", text)
        text = NE_FRENCH_PATTERN.sub(r"\1", text)
        text = NE_GERMAN_PATTERN.sub(r"\1", text)
    else:
        text = NE_PATTERN.sub(foreign_sym, text)
        text = NON_ENGLISH_PATTERN.sub(foreign_sym, text)
        text = NE_CZECH_PATTERN.sub(foreign_sym, text)
        text = NE_SLOVAK_PATTERN.sub(foreign_sym, text)
        text = NE_FRENCH_PATTERN.sub(foreign_sym, text)
        text = NE_GERMAN_PATTERN.sub(foreign_sym, text)

    if partial_sym != None:
        text = INTERRUPTED_PATTERN1.sub(partial_sym + " ", text)
    else:
        text = text.replace("-", "")

    for pair in COLLAPSE_WORDS:
        text = text.replace(pair[0], pair[1])

    text = WHITESPACE_PATTERN.sub(" ", text)
    text = text.strip()

    return text


def prepare_atco2(
    corpus_dir: Pathlike,
    output_dir: Optional[Pathlike] = None,
    foreign_sym: Optional[str] = "<unk>",
    partial_sym: Optional[str] = "<unk>",
    hesitation_sym: Optional[str] = "<unk>",
    unknown_sym: Optional[str] = "<unk>",
) -> Dict[str, Union[RecordingSet, SupervisionSet]]:
    """
    Returns the manifests which consist of the Recordings and Supervisions

    :param corpus_dir: Pathlike, the path of the data dir.
    :param output_dir: Pathlike, the path where to write the manifests.
    :param foreign_sym: str, foreign symbol.
    :param partial_sym: str, partial symbol. When set to None, will output partial words.
    :param hesitation_sym: str, hesitation symbol.
    :param unknown_sym: str, unknown symbol.
    :return: The RecordingSet and SupervisionSet with the keys 'audio' and 'supervisions'.
    """

    corpus_dir = Path(corpus_dir)
    assert corpus_dir.is_dir(), f"No such directory: {corpus_dir}"
    if output_dir is not None:
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

    xml_files = sorted((corpus_dir / "DATA").glob("*.xml"), key=lambda p: p.name)
    assert len(xml_files) == 560

    recordings = []
    supervisions = []

    from tqdm.auto import tqdm

    for xml_path in tqdm(xml_files, desc="Preparing"):
        root = ET.parse(xml_path).getroot()

        audio_path = xml_path.with_suffix(".wav")
        if not audio_path.is_file():
            logging.warning(f"No such file: {audio_path}")
            continue

        recording = Recording.from_file(audio_path)
        recordings.append(recording)

        for segment in root.findall(".//segment"):
            start_time = float(segment.findtext("start"))
            end_time = float(segment.findtext("end"))
            if end_time > recording.duration and end_time - recording.duration < 0.11:
                end_time = min(end_time, recording.duration)
            speaker = segment.findtext("speaker")
            text = segment.findtext("text")

            custom = {}
            for elem in segment.find("tags"):
                custom[elem.tag] = elem.findtext(".")
            custom["orig_text"] = text

            text = text_normalize(
                text,
                foreign_sym=foreign_sym,
                partial_sym=partial_sym,
                hesitation_sym=hesitation_sym,
                unknown_sym=unknown_sym,
            )

            supervision = SupervisionSegment(
                id=f"atco2_%s_%06d_%06d"
                % (audio_path.stem, start_time * 100, end_time * 100),
                recording_id=recording.id,
                start=start_time,
                duration=end_time - start_time,
                channel=0,
                language="English",
                text=text,
                speaker=speaker,
                custom=custom,
            )

            supervisions.append(supervision)

    recording_set = RecordingSet.from_recordings(recordings)
    supervision_set = SupervisionSet.from_segments(supervisions)

    validate_recordings_and_supervisions(recording_set, supervision_set)

    if output_dir is not None:
        supervision_set.to_file(output_dir / "atco2_supervisions_test.jsonl.gz")
        recording_set.to_file(output_dir / "atco2_recordings_test.jsonl.gz")

    return {"recordings": recording_set, "supervisions": supervision_set}
